raw_assessments = spark.read.format("kafka").option("kafka.bootstrap.servers", "kafka:29092").option("subscribe","assessments").option("startingOffsets", "earliest").option("endingOffsets", "latest").load()
raw_assessments.cache()
raw_assessments.printSchema()
assessments = raw_assessments.select(raw_assessments.value.cast('string'))
assessments.printSchema()
import sys
sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf8', buffering=1)
import json
extracted_assessments = assessments.rdd.map(lambda x: json.loads(x.value)).toDF()
extracted_assessments.show()
extracted_assessments.printSchema()
extracted_assessments.registerTempTable('assessments')
total_count = spark.select count(*) as count from assessmentssql(
total_count = spark.sql("select count(*) as count from assessments")
total_count.show()
total_count.write.parquet("/tmp/total_count")
learning_git = spark.select count(*) as learning_git_count from assessments where exam_(Learning (Gitname=sql(
learning_git = spark.sql("select count(*) as learning_git_count from assessments where exam_name='Learning Git'")
learning_git.show()
learning_git.write.parquet("/tmp/learning_git")
most_common = spark.sql("select exam_name, count(*) from assessments group by assessments.exam_name order by count(*) DESC limit 5")
most_common.show()
least_common = spark.sql("select exam_name, count(*) from assessments group by assessments.exam_name order by count(*) limit 5")
least_common.show()
least_common.write.parquet("/tmp/least_common")
most_common = spark.sql("select exam_name, count(*) as exam_count from assessments group by assessments.exam_name order by count(*) DESC limit 5")
most_common.show()
most_common.write.parquet("/tmp/most_common")
least_common = spark.sql("select exam_name, count(*) as exam_count from assessments group by assessments.exam_name order by count(*) limit 5")
least_common.show()
least_common.write.parquet("/tmp/least_common")
exam_start_times = spark.sql("select exam_name, started_at from assessments")
exam_start_times.show()
exam_start_times.write.parquet("/tmp/exam_start_times")
distinct_user_count = spark.sql("select count(distinct user_exam_id) as distinct_user_count from assessments")
disinct_user_count.show()
distinct_user_count.show()
distinct_user_count.write.parquet("/tmp/distinct_user_count")
attempts_per_exam = spark.sql("select exam_name, max_attempts from assessments group by exam_name")
attempts_per_exam = spark.sql("select exam_name, max_attempts from assessments")
attempts_per_exam.show()
docker-compose exec spark cat /root/.python_history
docker-compose exec spark cat /root/.python_history > myspark-history.txt
